{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dcabe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9315d2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['id', 'category', 'subcategory', 'title', 'abstract', 'url', 'entities', 'events']\n",
    "df = pd.read_csv(r'data/news.tsv', sep='\\t', names=column_names, header=None)\n",
    "df['content'] = df['title'].fillna('') + ' ' + df['abstract'].fillna('')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['clean_content'] = df['content'].apply(preprocess_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25048ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "tfidf_matrix = tfidf.fit_transform(df['clean_content'])\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebec6f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Without help from US, UN climate fund struggle...</td>\n",
       "      <td>Rich countries gathered Thursday in France to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>These Cranberry Sauce Recipes Are Perfect for ...</td>\n",
       "      <td>You'll never want the store-bought version eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Early symptoms of dementia: Be aware of subtle...</td>\n",
       "      <td>Would you be able to recognize the symptoms of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "30  Without help from US, UN climate fund struggle...   \n",
       "44  These Cranberry Sauce Recipes Are Perfect for ...   \n",
       "96  Early symptoms of dementia: Be aware of subtle...   \n",
       "\n",
       "                                             abstract  \n",
       "30  Rich countries gathered Thursday in France to ...  \n",
       "44  You'll never want the store-bought version eve...  \n",
       "96  Would you be able to recognize the symptoms of...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "liked_article_indices = [30,44,96] \n",
    "df.iloc[liked_article_indices][['title', 'abstract']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bf3884",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_profile_matrix = tfidf_df.iloc[liked_article_indices]\n",
    "\n",
    "user_profile_vector = user_profile_matrix.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553b0778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top interests of the user:\n",
      "symptom         0.221117\n",
      "sauce           0.124755\n",
      "fund            0.123864\n",
      "dementia        0.120002\n",
      "climate         0.119251\n",
      "recognize       0.118269\n",
      "bought          0.116473\n",
      "aware           0.114535\n",
      "version         0.113215\n",
      "dinner          0.105256\n",
      "recipe          0.098089\n",
      "perfect         0.097908\n",
      "thanksgiving    0.097162\n",
      "store           0.090336\n",
      "ever            0.088664\n",
      "able            0.087674\n",
      "help            0.086526\n",
      "never           0.086171\n",
      "want            0.077393\n",
      "un              0.075004\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "top_terms = user_profile_vector.sort_values(ascending=False).head(20)\n",
    "print(\"Top interests of the user:\")\n",
    "print(top_terms)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
